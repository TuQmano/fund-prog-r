---
title: "Fundamentos de la programación estadística y Data Mining en R"
author: "Dr. Germán Rosati (Digital House - UNTREF - UNSAM)"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: slidy_presentation
subtitle: "Unidad 3. Regresión Logística y Árboles de Decisión en R"
---

## Implementación de un árbol de decisión mediante CART en R
Usaremos la librería `tree` para construir árboles de decisión y clasificación. Vamos a trabajar con el dataset `carseats`.
En este dataset `Sales` (la variable dependiente) es una variable continua. Entonces, vamos a usar `ifelse()` para recodificar en dos valores. Y la agregamos a `Carseats`
```{r, tidy=TRUE}
library(tree)
library(ISLR)
attach(Carseats )
High <- ifelse(Sales <= 8,"No","Yes")
Carseats <- data.frame(Carseats ,High)
```
Ahora usamos la función `tree` para generar un árbol de clasificación que prediga la variable nueva (dicotómica que acabamos de crear). Como vamos a ver la sintaxis de `tree()` es muy similar a la de `lm()` y `glm()`.
```{r, tidy=TRUE}
tree.carseats <- tree(High~. -Sales, data = Carseats)
summary(tree.carseats)
```
Vemos que el Training Error es de 9%. En árboles de clasificación lo que reporta la función como "mean deviance" en el output de `summary()` es:
$-2\sum_{m}\sum_{k}n_{mk}*log*\hat{p}_{mk}$
donde $n_{mk}$ es el número de observaciones en la m-ésima terminal que corresponde a la k-ésima clase. Una desviación pequeña indica un árbol que provee un buen ajuste para los datos (de entrenamiento). La resdial mean deviance es implemente ese valor dividido por $n-|T_{0}|$.

## Implementación de un árbol de decisión mediante CART en R: gráficos
Una cosa interesante y útil que proveen los árboles de decisión es que permiten tener una salida gráfica bastante útil e intuitiva del modelo. Usamos la función `plot()` para graficar la estructura y `text()` para agregarle las "etiquetas". El argumento `pretty=0` le dice a R que muestre las categorías completas de cada cualquier predictor cualitativo.
```{r, tidy=TRUE}
plot(tree.carseats)
text(tree.carseats, pretty = 0)
```
La ubicación en las repisas (`shelving location`) parece ser el mejor predictor de `Sales`, dado que la primera rama del árbol diferencia las buenas locaciones de las malas y medianas.
Si tipeamos el nombre del objeto árbol R imprime un output correspondiente a cada rama. Imprime, además, el criterio de splitting (`Price` < 92.5), la cantidad de observciones en esa rama, el desvío, la predicción general de la rama (`Yes` o `No`) y la fracción de observaciones dentro de esa rama que toman valores `Yes` y `No`. Las ramas que llegan hasta las terminales aparecen con asteriscos.

```{r, tidy=TRUE}
tree.carseats
```

## Implementación de un árbol de decisión mediante CART en R: training y test sets
Para evaluar la performance de clasificacione en estos datos, tensmo sque esimar el test error. Dividimos las observaciones en test y training sets, creamos un árbol en el training set y evaluamos en el test-set. La función `predict()` se puede usar (de la misma forma que el `glm()` o `lm()`). En el caso de un árbol de clasificación el argumento `type="class"` le dice a R que devuelva la categoría predicha. En este caso, vemos que la tasa de predicciones correctas cambia a 71.5 % sobre el test set.
```{r, tidy=TRUE}
set.seed(2)
train <- sample(1:nrow(Carseats), 200)
Carseats.test <- Carseats [-train,]
High.test <- High[-train]
tree.carseats <- tree(High~.-Sales, Carseats, subset = train)
tree.pred <- predict(tree.carseats, Carseats.test, type = "class")
table(tree.pred, High.test)
```
Luego, podemos pensar en "podar" el árbol para obtener mejores resultados. la función `cv.tree()` realiza una validación cruzada para determinar el nivel óptimo de complejidad del árbol. "Cost complexity prunning" (podado basado en costo-complejidad) se usa para seleccinar una secuencia de árboles. Usamos el argumento `FUN = prune` para indicar que queremos que el error de clasificación sea el valor que se use para el proceso de podado. La función `cv.tree()` usa por defecto el desvío. A su vez, `cv.tree()` reporta la cantidad de nodos terminales en cada árbol considerado (size), así como el parámetro de costo-complejidad usado (k que se corresponde con lo que llamamos antes $\alpha$).
```{r, tidy=TRUE}
set.seed(3)
cv.carseats <- cv.tree(tree.carseats, FUN=prune.misclass)
names(cv.carseats)
```
Notar que, a pesar del nombre `dev` corresponde a la tasa de error "cros-validada". El árbol con 9 nodos terminales es el que tiene el menor error de clasificación "cros-validado" (50). Graficamos la tasa de error como una función del tamaño del árbol y de k.
```{r, tidy=TRUE}
par(mfrow=c(1,2))
plot(cv.carseats$size, cv.carseats$dev, type="b")
plot(cv.carseats$k, cv.carseats$dev, type="b")
```
Ahora, podemos aplciar`prune.misclass()`para podar el árbol y quedarnos con el de 9 nodos.
```{r, tidy=TRUE}
prune.carseats <- prune.misclass(tree.carseats, best = 9)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
```
¿Qué tan bien funciona este árbol de 9 nodos en el test-set? Usamos de nuevo `predict()`: 
```{r, tidy=TRUE}
tree.pred <- predict(prune.carseats, Carseats.test, type="class")
table(tree.pred, High.test)
```
Ahora, el 77% de las observaciones están bien clasificadas. Es decir, que este proceso de cost-complexity prunning generó no solamente un árbol más interpretable sino que además mejoró la performance preditiva. Si incrementamos el valor de `best`, obtenemos un árbol más grande y con peor performance de clasificación.
```{r, tidy=TRUE}
prune.carseats <- prune.misclass(tree.carseats, best = 15)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
tree.pred <- predict(prune.carseats, Carseats.test, type="class")
table(tree.pred, High.test)
```
